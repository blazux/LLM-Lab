# ğŸ§  LLM-Lab

*A minimalistic tool to build, train, and possibly break your own LLM â€” locally, gloriously, and without safety nets.*

---

## ğŸ§© Requirements

This tool assumes you have:

- A working **CUDA environment** with `bfloat16` support (mandatory).  
- A **basic understanding of what youâ€™re doing** (optional but recommended).  

If something goes wrong, youâ€™ll know.  
If you donâ€™t know, youâ€™ll learn.  
Thatâ€™s how science works.

Any mistake will result in a **beautiful, educational CUDA or Torch error**.  
Take it as feedback from the universe.

---

## ğŸ§± Model Configuration

Define what your model is made of â€” think of it as Lego for nerds.

### ğŸ§¬ Choose your components:
- **Tokenizer** â†’ any on HuggingFace (you break it, you fix it)  
- **Positional Encoding** â†’ `sinusoidal`, `rope`, `alibi`, `yarn`  
- **Normalization** â†’ `layernorm`, `rmsnorm`  
- **Attention** â†’ `Multi-Headed`, `Multi-Query`, `Grouped Query`  
- **Feed Forward Activation** â†’ `ReLU`, `GeLU`, `Leaky ReLU`, `SiLU`, `SwiGLU`  

### âš™ï¸ Define model size:
- `d_model` (embedding size) must be divisible by the number of heads â€” because maths.  
- For Grouped Query Attention: number of attention heads must be divisible by `n_kv_heads`.  

Once your architecture is defined, the config is saved in **Hugging Face format**.

---

## ğŸ”¥ Base Training (Not calling it Pretraining to avoid killing motivation)

Welcome to the world of gradient descent and existential doubt.

### Configure your training:
- **Steps** â†’ the more, the merrier (and slower).  
- **Optimizer** â†’ `AdamW`, `Adafactor`, `Lion`, `Sophia`, `Muon`.  
- **Scheduler** â†’ `None`, `Cosine`, `Linear decay`, `Polynomial decay`.  
- **Hyperparams** â†’ learning rate, batch size, gradient accumulation, clipping, etc.  
- **Datasets** â†’ any on HuggingFace. Multiple datasets are automatically interleaved for you (youâ€™re welcome).

Then, press *train* and watch numbers go down.  
Or not.

---

## ğŸ“ SFT (Supervised Fine-Tuning)

Now that your model knows â€œlanguage,â€ letâ€™s teach it to **follow instructions** instead of screaming random tokens.

SFT fine-tunes your pretrained model on **instruction / dialogue datasets** â€” the kind where humans pretend to be helpful.

### Setup:
- **Base model** â†’ checkpoint from pretraining (weights only; optimizer state is reset).  
- **Optimizer / Scheduler / Hyperparams** â†’ same as pretraining (smaller learning rate is wise).  
- **Training steps** â†’ ~5kâ€“10k, depending on how stubborn your model is.  
- **Datasets** â†’ things like `HuggingFaceTB/smoltalk2`, `OpenAssistant/oasst1`, or any instruction dataset.

The token counter resets. The suffering does not.

---

## ğŸ§  RLHF (Reinforcement Learning from Human Feedback)

Because supervised fine-tuning teaches â€œwhat to say,â€  
but **RLHF teaches â€œhow to say it to please humans.â€**

Currently supports **PPO** and **DPO**, the two main schools of reinforcement enlightenment.

---

### ğŸ¥Š PPO (Proximal Policy Optimization)

The â€œgym broâ€ of training â€” all about steps, rewards, and regret minimization.

- **Reward model** â†’ pick one from HuggingFace (`OpenAssistant/reward-model-deberta-v3-large-v2` is a good start).  
- **Batch / Mini-batch / Epochs / LR** â†’ you know the drill.  
- **Gamma** â†’ how much the model cares about the future (discount factor).  
- **Lambda** â†’ how smooth the advantage estimation is (0.9 = impulsive, 1.0 = zen).  
- **Dataset** â†’ as usual, any text dataset will do â€” just make sure itâ€™s relevant.

Expect slow improvement, occasional reward spikes, and philosophical questions like *â€œis this actually working?â€*

---

### ğŸ§˜ DPO (Direct Preference Optimization)

PPOâ€™s calmer, mathier cousin.  
No sampling loops, no KL penalties â€” just clean, supervised gradients based on preferences.

- **Reference model** â†’ usually your SFT model (or leave blank to use the same).  
- **Batch / Mini-batch / LR / etc.** â†’ same logic as before.  
- **Beta** â†’ controls how strongly the model obeys the reward (too high = robotic, too low = chaotic).  
- **Dataset** â†’ must include human preference pairs (good/bad responses).

DPO is simple, elegant, and 80% less likely to explode.  
But that last 20% still exists.

---

## ğŸ§ª Model Testing

Once your model has been pretrained, fine-tuned, reinforced, or not â€” at any point you want in fact... 
you can finally **talk to it**.

- Load your best checkpoint  
- Type a prompt  
- Watch your digital creation respond 

If it hallucinates, thatâ€™s part of the charm.

---

## ğŸ’­ Philosophy

- **No magic, no automation, no safety nets.**  
- You configure everything, you run everything, you break everything.  
- Every crash is a feature â€” a step toward enlightenment.  

> â€œYou donâ€™t need 10,000 GPUs to train a model.  
> You just need curiosity, patience, and a high pain tolerance.â€

---

## âš–ï¸ License

MIT â€” because chaos should be open-source.